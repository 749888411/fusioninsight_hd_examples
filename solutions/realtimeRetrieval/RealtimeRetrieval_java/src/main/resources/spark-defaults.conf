spark.sql.bigdata.sensitive.data = pwd,password
spark.password.factory = com.huawei.spark.client.ClientDecode
spark.rdd.compress = false
spark.yarn.report.interval = 1000
spark.streaming.receiver.writeAheadLog.maxFailures = 3
spark.shuffle.io.sendBuffer = -1
spark.sql.cbo.joinReorder.card.weight = 0.7
spark.sql.ui.retainedExecutions = 1000
spark.shuffle.compress = true
spark.dynamicAllocation.minExecutors = 0
spark.streaming.kafka.reliability = false
spark.logLineage = false
spark.task.maxFailures = 4
spark.locality.wait.rack = 3000
spark.shuffle.io.maxRetries = 12
spark.ssl.ui.protocol = TLSv1.1,TLSv1.2
spark.blacklist.enabled = true
spark.kryoserializer.buffer.max = 64MB
spark.sql.statistics.histogram.numBins = 254
spark.storage.replication.policy = org.apache.spark.storage.RandomBlockReplicationPolicy
spark.streaming.kafka.maxRetries = 1
spark.yarn.am.extraJavaOptions = -Dlog4j.configuration=./__spark_conf__/__hadoop_conf__/log4j-executor.properties -Djava.security.auth.login.config=./__spark_conf__/__hadoop_conf__/jaas-zk.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=./__spark_conf__/__hadoop_conf__/kdc.conf -Djdk.tls.ephemeralDHKeySize=2048
spark.yarn.am.waitTime = 100s
spark.extraListeners = org.apache.spark.fi.listeners.SparkAMRegistedListener,org.apache.spark.util.HWSparkListener
spark.streaming.receiverRestartDelay = 2000
spark.shuffle.statistics.verbose = false
spark.shuffle.servicev2.port = 27338
spark.shuffle.memoryFraction = 0.2
spark.speculation.multiplier = 1.5
spark.ui.retainedJobs = 1000
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version = 2
spark.ssl.historyServer.enabled = true
spark.eventLog.logBlockUpdates.enabled = false
spark.sql.bigdata.register.dialect = org.apache.spark.sql.hbase.HBaseSQLParser,org.apache.spark.sql.CarbonInternalSqlParser
spark.rpc.io.threads = 0
spark.shuffle.manager = SORT
spark.streaming.kafka.direct.lifo = false
spark.eventlog.group.time = 2h
spark.reducer.maxSizeInFlight = 48MB
spark.sql.decimalOperations.allowPrecisionLoss = true
spark.streaming.ui.retainedBatches = 1000
spark.sql.function.eltOutputAsString = false
spark.log.callerContext = none
spark.dynamicAllocation.initialExecutors = 0
spark.blacklist.application.maxFailedExecutorsPerNode = 2
spark.sql.hive.implementation = org.apache.spark.sql.hive.HiveACLClientImpl
spark.executor.memory = 4G
spark.sql.data.skew.min.time = 300000
spark.sql.catalog.class = org.apache.spark.sql.hive.HiveACLExternalCatalog
spark.shuffle.sort.bypassMergeThreshold = 200
spark.scheduler.minRegisteredResourcesRatio = 0.8
spark.rpc.numRetries = 3
spark.logOptimization.enable = true
spark.sql.bigdata.register.preExecutionRule = org.apache.spark.sql.hbase.execution.AddCoprocessor$,org.apache.spark.sql.execution.EnsureRowFormats$,org.apache.spark.sql.hive.CarbonPrivCheck
spark.sql.adaptive.minNumPostShufflePartitions = 1
spark.eventLog.compress = false
spark.sql.autoBroadcastJoinThreshold = 10485760
spark.port.maxRetries = 16
spark.serializer.objectStreamReset = 100
spark.yarn.am.memory = 1G
spark.executor.extraClassPath = 
spark.python.worker.reuse = true
spark.sql.broadcastTimeout = 300
spark.ssl.ui.enabledAlgorithms = TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,TLS_DHE_DSS_WITH_AES_256_GCM_SHA384,TLS_DHE_RSA_WITH_AES_256_CBC_SHA256,TLS_DHE_DSS_WITH_AES_256_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,TLS_DHE_DSS_WITH_AES_128_CBC_SHA256,TLS_DHE_RSA_WITH_AES_128_GCM_SHA256,TLS_DHE_DSS_WITH_AES_128_GCM_SHA256
spark.admin.acls = admin
spark.sql.parquet.filterPushdown = true
spark.sql.files.maxRecordsPerFile = 0
spark.task.reaper.killTimeout = -1
spark.sql.adaptive.maxNumPostShufflePartitions = 500
spark.yarn.containerLauncherMaxThreads = 25
spark.yarn.queue = default
spark.io.compression.snappy.blockSize = 32KB
spark.sql.dialect = org.apache.spark.sql.hive.huawei.BigSQLDialect
spark.yarn.driver.memoryOverhead = 1024
spark.driver.cores = 1
spark.network.timeoutInterval = 60
spark.security.hideInfo.enabled = true
spark.driver.extraLibraryPath = /opt/huawei/Bigdata/FusionInsight_HD_6.5.1/install/FusionInsight-Hadoop-3.1.1/hadoop/lib/native
spark.sql.inMemoryColumnarStorage.partitionPruning = true
spark.sql.adaptive.skewedPartitionRowCountThreshold = 10000000
spark.files.fetchTimeout = 60
spark.shuffle.registration.maxAttempts = 3
spark.ui.view.acls = 
spark.stage.maxConsecutiveAttempts = 4
spark.yarn.kerberos.relogin.period = 1m
spark.insertOverwrite.trash.interval = 1440
spark.sql.bigdata.register.extendedResolutionRule = org.apache.spark.sql.hbase.catalyst.analysis.ReplaceOutput$,org.apache.spark.sql.hive.CarbonPreInsertionCasts
spark.sql.cbo.joinReorder.dp.threshold = 12
spark.storage.cachedPeersTtl = 60000
spark.yarn.security.credentials.hbase.enabled = false
spark.buffer.size = 65536
spark.storage.replication.topologyMapper = org.apache.spark.storage.DefaultTopologyMapper
spark.shuffle.io.retryWait = 5s
spark.streaming.blockInterval = 200ms
spark.ui.retainedStages = 1000
spark.eventLog.buffer.kb = 100K
spark.driver.extraJavaOptions = -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:-OmitStackTraceInFastThrow -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=20 -XX:GCLogFileSize=10M -Dlog4j.configuration=./__spark_conf__/__hadoop_conf__/log4j-executor.properties -Djava.security.auth.login.config=./__spark_conf__/__hadoop_conf__/jaas-zk.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=./__spark_conf__/__hadoop_conf__/kdc.conf -Djetty.version=x.y.z -Dorg.xerial.snappy.tempdir=/opt/huawei/Bigdata/tmp/spark2x_app -Dcarbon.properties.filepath=./__spark_conf__/__hadoop_conf__/carbon.properties -Djdk.tls.ephemeralDHKeySize=2048
spark.eventLog.overwrite = false
spark.sql.task.timeout.threshold.value = 86400000
spark.sql.statistics.ndv.maxError = 0.05
spark.dynamicAllocation.cachedExecutorIdleTimeout = 120
spark.shuffle.spill.compress = true
spark.blacklist.stage.maxFailedExecutorsPerNode = 2
spark.sql.large.query.results.threshold.value = 0.2
spark.monitor.jmx.serverClass = com.huawei.spark.monitor.JMXServer
spark.random.port.min = 22600
spark.speculation.quantile = 0.75
spark.sql.adaptive.skewedJoin.enabled = true
spark.blacklist.timeout = 1h
spark.internal.version = 6.5.1
spark.sql.files.ignoreCorruptFiles = false
spark.eventlog.group.enable = true
spark.executor.cores = 1
spark.dynamicAllocation.maxExecutors = 2048
spark.shuffle.sort.kvChunkSize = 4194304
spark.rpc.io.numConnectionsPerPeer = 1
spark.thriftserver.zookeeper.namespace = /sparkthriftserver2x
spark.storage.maxReplicationFailures = 1
spark.sql.statistics.percentile.accuracy = 10000
spark.task.cpus = 1
spark.speculation.interval = 100
spark.sql.bigdata.show.query = true
spark.executor.userClassPathFirst = false
spark.shuffle.file.buffer = 32KB
spark.shuffle.service.enabled = false
spark.sql.sources.maxConcurrentWrites = 5
spark.sql.authorization.enabled = true
spark.scheduler.revive.interval = 1000
spark.driver.supervise = false
spark.buffer.pageSize = 16m
spark.blacklist.task.maxTaskAttemptsPerNode = 2
spark.sql.inMemoryColumnarStorage.batchSize = 10000
spark.sql.session.state.builder = org.apache.spark.sql.hive.FIHiveACLSessionStateBuilder
spark.sql.function.concatBinaryAsString = false
spark.scheduler.listenerbus.eventqueue.capacity = 1000000
spark.network.timeout = 360s
spark.sql.adaptive.join.enabled = true
spark.sql.bigdata.register.optimizeRuleTail = org.apache.spark.sql.optimizer.ResolveCarbonFunctions
spark.server.name = Spark2x
spark.sql.shuffle.partitions = 200
spark.storage.memoryMapThreshold = 2m
spark.sql.orc.impl = hive
spark.shuffle.io.numConnectionsPerPeer = 1
spark.sql.hive.advancedPartitionPredicatePushdown.enabled = true
spark.sql.adaptive.skewedPartitionSizeThreshold = 67108864
spark.kryo.registrationRequired = false
spark.sql.cbo.joinReorder.enabled = false
spark.yarn.historyServer.address = 189.154.5.82:20026/Spark2x/JobHistory2x/70
spark.cleaner.referenceTracking.blocking.shuffle = false
spark.yarn.executor.memoryOverhead = 1024
spark.sql.statistics.histogram.enabled = false
spark.streaming.driver.writeAheadLog.rollingIntervalSecs = 60
spark.ui.retainedDeadExecutors = 100
spark.logConf = false
spark.yarn.archive = hdfs://hacluster/user/spark2x/jars/6.5.1/spark-archive-2x.zip
spark.executor.heartbeatInterval = 200
spark.yarn.am.cores = 1
spark.sql.parser.escapedStringLiterals = true
spark.broadcast.blockSize = 4096
spark.sql.hive.convertMetastoreOrc = true
spark.yarn.scheduler.heartbeat.interval-ms = 3000
spark.sql.tungsten.enabled = true
spark.carbon.storePath = hdfs://hacluster/user/hive/warehouse/carbon.store
spark.streaming.driver.writeAheadLog.maxFailures = 3
spark.rpc.lookupTimeout = 120s
spark.sql.adaptive.shuffle.targetPostShuffleInputSize = 26214400
spark.sql.parquet.compression.codec = snappy
spark.task.reaper.pollingInterval = 10s
spark.storage.memoryFraction = 0.6
spark.mapOutputStatus.maxShuffleMapStatusSize = 256M
spark.rpc.retry.wait = 3s
spark.sql.orc.enableVectorizedReader = true
spark.carbon.sqlastbuilder.classname = org.apache.spark.sql.hive.CarbonInternalSqlAstBuilder
spark.dynamicAllocation.enabled = false
spark.yarn.access.hadoopFileSystems = hdfs://hacluster,hdfs://hacluster
spark.files.overwrite = false
spark.locality.wait.node = 3000
spark.driver.extraClassPath = /opt/huawei/Bigdata/common/runtime/security
spark.hadoop.validateOutputSpecs = true
spark.streaming.receiver.blockStoreTimeout = 30
spark.driver.userClassPathFirst = false
spark.kryo.referenceTracking = true
spark.blacklist.stage.maxFailedTasksPerExecutor = 2
spark.deploy.zookeeper.url = 189.154.5.75:24002,189.154.5.72:24002,189.154.7.48:24002
spark.sql.codegen.maxFields = 100
spark.locality.wait = 3000
spark.streaming.concurrentJobs = 1
spark.dynamicAllocation.executorIdleTimeout = 60s
spark.broadcast.factory = org.apache.spark.broadcast.TorrentBroadcastFactory
spark.storage.unrollFraction = 0.2
spark.rpc.connect.threads = 64
spark.blacklist.killBlacklistedExecutors = false
spark.sql.parquet.binaryAsString = false
spark.cleaner.referenceTracking.cleanCheckPoints = false
spark.dynamicAllocation.schedulerBacklogTimeout = 1s
spark.solr.obtainToken.enabled = false
spark.cleaner.referenceTracking = true
spark.sql.caseSensitive = false
spark.sql.sources.partitionOverwriteMode = STATIC
spark.io.compression.lz4.blockSize = 32KB
spark.sql.hiveClient.isolation.enabled = false
spark.sql.codegen.wholeStage = true
spark.shuffle.registration.timeout = 5000
spark.sql.inMemoryColumnarStorage.compressed = true
spark.scheduler.mode = FIFO
spark.yarn.dist.innerarchives = hdfs://hacluster/user/spark2x/jars/6.5.1/spark-archive-2x-x86.zip#x86,hdfs://hacluster/user/spark2x/jars/6.5.1/spark-archive-2x-arm.zip#arm
spark.shuffle.consolidateFiles = false
spark.shuffle.spill.batchSize = 10000
spark.eventlog.group.size = 10g
spark.local.dir = #{tmp_dir}/spark/localDir
spark.sql.sources.parallelPartitionDiscovery.threshold = 32
spark.rpc.askTimeout = 120s
spark.sql.locale.support = false
spark.executor.extraLibraryPath = /opt/huawei/Bigdata/FusionInsight_HD_6.5.1/install/FusionInsight-Hadoop-3.1.1/hadoop/lib/native
spark.shuffle.spill = true
spark.ui.customErrorPage = true
spark.httpdProxy.enable = true
spark.ui.memoryFraction = 0.1
spark.beeline.principal = spark2x/hadoop.hadoop.com@HADOOP.COM
spark.sql.adaptive.enabled = true
spark.sql.relationCache.skip = 
spark.locality.wait.process = 3000
spark.sql.parquet.cacheMetadata = true
spark.authenticate.secret = 
spark.shuffle.service.index.cache.size = 100M
spark.sql.objectHashAggregate.sortBased.fallbackThreshold = 128
spark.sql.bigdata.register.strategyRule = org.apache.spark.sql.hbase.DummySparkPlanner,org.apache.spark.sql.hive.CarbonStrategy,org.apache.spark.sql.hive.CarbonDDLStrategy
spark.scheduler.maxRegisteredResourcesWaitingTime = 30000
spark.driver.memory = 4G
spark.logout.enabled = true
spark.shuffle.sasl.timeout = 120s
spark.sql.adaptive.skewedPartitionFactor = 10
spark.ui.port = 0
spark.ui.killEnabled = true
spark.io.compression.codec = lz4
spark.speculation = false
spark.admin.acls.groups = 
spark.task.reaper.enabled = false
spark.sql.execution.useObjectHashAggregateExec = true
spark.modify.acls.groups = 
spark.blacklist.task.maxTaskAttemptsPerExecutor = 1
spark.memory.useLegacyMode = false
spark.task.reaper.threadDump = true
spark.sql.bigdata.initFunction = org.apache.spark.sql.hbase.HBaseEnv,org.apache.spark.sql.CarbonEnv
spark.sql.cbo.enabled = false
spark.cleaner.referenceTracking.blocking = true
spark.modify.acls = 
spark.blacklist.application.fetchFailure.enabled = false
spark.ui.showConsoleProgress = true
spark.blacklist.application.maxFailedTasksPerExecutor = 2
spark.sql.hive.verifyPartitionPath = false
spark.shuffle.io.receiveBuffer = -1
spark.redaction.regex = (?!)secret|password
spark.ui.view.acls.groups = 
spark.sql.execution.pandas.respectSessionTimeZone = true
spark.sql.statistics.size.autoUpdate.enabled = false
spark.storage.replication.proactive = false
spark.eventLog.dir = hdfs://hacluster/spark2xJobHistory2x
spark.serializer = org.apache.spark.serializer.JavaSerializer
spark.kryoserializer.buffer = 64KB
spark.localExecution.enabled = false
spark.sql.bigdata.register.analyseRule = org.apache.spark.sql.hive.acl.CarbonAccessControlRules
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = 1s
spark.shuffle.io.mode = NIO
spark.shuffle.io.preferDirectBufs = true
spark.rpc.message.maxSize = 128
spark.streaming.receiver.writeAheadLog.enable = false
spark.executor.periodicGC.interval = 30min
spark.acls.enable = true
spark.yarn.submit.waitAppCompletion = true
spark.cleaner.periodicGC.interval = 30min
spark.authenticate = true
spark.ui.threadDumpsEnabled = true
spark.streaming.blockQueueSize = 10
spark.random.port.max = 22899
spark.sql.parquet.int96AsTimestamp = true
spark.sql.adaptive.shuffle.targetPostShuffleRowCount = 20000000
spark.inputFormat.cache.enabled = true
spark.eventLog.enabled = true
spark.streaming.unpersist = true
spark.executor.extraJavaOptions = -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:-OmitStackTraceInFastThrow -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=20 -XX:GCLogFileSize=10M -Dlog4j.configuration=./__spark_conf__/__hadoop_conf__/log4j-executor.properties -Djava.security.auth.login.config=./__spark_conf__/__hadoop_conf__/jaas-zk.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=./__spark_conf__/__hadoop_conf__/kdc.conf -Dcarbon.properties.filepath=./__spark_conf__/__hadoop_conf__/carbon.properties -Djdk.tls.ephemeralDHKeySize=2048
