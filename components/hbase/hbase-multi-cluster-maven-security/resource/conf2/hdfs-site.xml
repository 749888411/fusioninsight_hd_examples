<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<configuration>
<property>
<name>ipc.server.tcpnodelay</name>
<value>false</value>
</property>
<property>
<name>dfs.namenode.fs-limits.max-blocks-per-file</name>
<value>1048576</value>
</property>
<property>
<name>dfs.client.https.need-auth</name>
<value>false</value>
</property>
<property>
<name>dfs.replication</name>
<value>3</value>
</property>
<property>
<name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
<value>true</value>
</property>
<property>
<name>dfs.ha.fencing.ssh.private-key-files</name>
<value>/home/omm/.ssh/id_rsa</value>
</property>
<property>
<name>backup.meta.zk.acl</name>
<value>world:anyone:r,auth::cdrwa</value>
</property>
<property>
<name>dfs.ha.zkfc.port</name>
<value>25015</value>
</property>
<property>
<name>dfs.namenode.kerberos.https.principal</name>
<value>hdfs/hadoop.hdfs.com@HDFS.COM</value>
</property>
<property>
<name>net.topology.node.switch.mapping.impl</name>
<value>org.apache.hadoop.net.ScriptBasedMapping</value>
</property>
<property>
<name>oi.dfs.colocation.zookeeper.session-timeout.ms</name>
<value>45000</value>
</property>
<property>
<name>ipc.server.handler.queue.size</name>
<value>100</value>
</property>
<property>
<name>dfs.namenode.journalnode</name>
<value>160-157-0-10:25012;160-157-0-9:25012;167-52-0-13:25012</value>
</property>
<property>
<name>oi.dfs.colocation.zookeeper.quorum</name>
<value>160-157-0-10:24002,160-157-0-9:24002,167-52-0-13:24002</value>
</property>
<property>
<name>backup.meta.zk.parent-znode</name>
<value>/hdfs-backup</value>
</property>
<property>
<name>dfs.namenode.replication.min</name>
<value>1</value>
</property>
<property>
<name>dfs.namenode.num.checkpoints.retained</name>
<value>3</value>
</property>
<property>
<name>dfs.client.socketcache.expiryMsec</name>
<value>900</value>
</property>
<property>
<name>dfs.namenode.fs-limits.min-block-size</name>
<value>1048576</value>
</property>
<property>
<name>dfs.namenode.http.policy</name>
<value>HTTPS_ONLY</value>
</property>
<property>
<name>dfs.namenode.acls.enabled</name>
<value>true</value>
</property>
<property>
<name>dfs.namenode.datanode.registration.ip-hostname-check</name>
<value>true</value>
</property>
<property>
<name>dfs.namenode.path.based.cache.block.map.allocation.percent</name>
<value>0.25f</value>
</property>
<property>
<name>dfs.journalnode.kerberos.principal</name>
<value>hdfs/hadoop.hdfs.com@HDFS.COM</value>
</property>
<property>
<name>dfs.namenode.edits.noeditlogchannelflush</name>
<value>false</value>
</property>
<property>
<name>dfs.permissions.enabled</name>
<value>true</value>
</property>
<property>
<name>ha.zookeeper.quorum</name>
<value>160-157-0-10:24002,160-157-0-9:24002,167-52-0-13:24002</value>
</property>
<property>
<name>dfs.namenode.rpc-address.hacluster.16</name>
<value>160-157-0-10:25000</value>
</property>
<property>
<name>hadoop.proxyuser.miner.groups</name>
<value>*</value>
</property>
<property>
<name>ipc.server.read.threadpool.size</name>
<value>1</value>
</property>
<property>
<name>dfs.namenode.stale.datanode.interval</name>
<value>30000</value>
</property>
<property>
<name>dfs.namenode.rpc-address.hacluster.17</name>
<value>160-157-0-9:25000</value>
</property>
<property>
<name>backup.worker.implement.multi-thread.num</name>
<value>3</value>
</property>
<property>
<name>dfs.http.policy</name>
<value>HTTPS_ONLY</value>
</property>
<property>
<name>dfs.client.failover.proxy.provider.hacluster</name>
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
<property>
<name>dfs.namenode.replication.interval</name>
<value>3</value>
</property>
<property>
<name>dfs.namenode.safemode.min.datanodes</name>
<value>0</value>
</property>
<property>
<name>dfs.namenode.http.port</name>
<value>25002</value>
</property>
<property>
<name>dfs.namenode.https-address.hacluster.16</name>
<value>160-157-0-10:25003</value>
</property>
<property>
<name>dfs.namenode.https-address.hacluster.17</name>
<value>160-157-0-9:25003</value>
</property>
<property>
<name>dfs.datanode.kerberos.principal</name>
<value>hdfs/hadoop.hdfs.com@HDFS.COM</value>
</property>
<property>
<name>dfs.domain.socket.path</name>
<value>/home/omm/dn_socket</value>
</property>
<property>
<name>oi.dfs.colocation.zookeeper.lock.limit</name>
<value>10</value>
</property>
<property>
<name>dfs.namenode.handler.count</name>
<value>64</value>
</property>
<property>
<name>dfs.qjournal.write-txns.timeout.ms</name>
<value>20000</value>
</property>
<property>
<name>dfs.image.transfer.bandwidthPerSec</name>
<value>0</value>
</property>
<property>
<name>dfs.replication.max</name>
<value>512</value>
</property>
<property>
<name>dfs.webhdfs.enabled</name>
<value>true</value>
</property>
<property>
<name>dfs.namenode.delegation.token.max-lifetime</name>
<value>604800000</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>/srv/BigData/namenode</value>
</property>
<property>
<name>dfs.namenode.kerberos.principal</name>
<value>hdfs/hadoop.hdfs.com@HDFS.COM</value>
</property>
<property>
<name>dfs.namenode.avoid.write.stale.datanode</name>
<value>true</value>
</property>
<property>
<name>dfs.namenode.image.backup.enable</name>
<value>true</value>
</property>
<property>
<name>ipc.client.kill.max</name>
<value>10</value>
</property>
<property>
<name>oi.dfs.colocation.balancer.exclude.block.policy.class</name>
<value>com.huawei.hadoop.oi.colocation.PathPatternBasedExcludeBlockPolicy</value>
</property>
<property>
<name>dfs.namenode.num.extra.edits.retained</name>
<value>0</value>
</property>
<property>
<name>ipc.client.connect.max.retries.on.timeouts</name>
<value>15</value>
</property>
<property>
<name>dfs.block.access.token.enable</name>
<value>true</value>
</property>
<property>
<name>dfs.blocksize</name>
<value>134217728</value>
</property>
<property>
<name>dfs.namenode.image.backup.nums</name>
<value>24</value>
</property>
<property>
<name>com.sas.lasr.configs</name>
<value></value>
</property>
<property>
<name>dfs.encrypt.data.transfer</name>
<value>false</value>
</property>
<property>
<name>dfs.namenode.write.stale.datanode.ratio</name>
<value>0.5f</value>
</property>
<property>
<name>dfs.client-write-packet-size</name>
<value>262144</value>
</property>
<property>
<name>dfs.namenode.checkpoint.txns</name>
<value>1000000</value>
</property>
<property>
<name>dfs.client.block.write.retries</name>
<value>3</value>
</property>
<property>
<name>dfs.client.failover.max.attempts</name>
<value>10</value>
</property>
<property>
<name>dfs.namenode.safemode.threshold-pct</name>
<value>0.999</value>
</property>
<property>
<name>dfs.client.block.write.replace-datanode-on-failure.replication</name>
<value>2</value>
</property>
<property>
<name>dfs.journalnode.kerberos.internal.spnego.principal</name>
<value>HTTP/_HOST@HDFS.COM</value>
</property>
<property>
<name>dfs.hosts</name>
<value></value>
</property>
<property>
<name>dfs.client.read.shortcircuit.skip.checksum</name>
<value>true</value>
</property>
<property>
<name>dfs.namenode.kerberos.principal.pattern</name>
<value>*</value>
</property>
<property>
<name>dfs.nameservices.mappings</name>
<value>[{"name":"hacluster","roleInstances":["16","17"]}]</value>
</property>
<property>
<name>dfs.namenode.http-address.hacluster.17</name>
<value>160-157-0-9:25002</value>
</property>
<property>
<name>dfs.namenode.replication.max-streams</name>
<value>2</value>
</property>
<property>
<name>dfs.namenode.http-address.hacluster.16</name>
<value>160-157-0-10:25002</value>
</property>
<property>
<name>dfs.nameservices</name>
<value>hacluster</value>
</property>
<property>
<name>ipc.client.idlethreshold</name>
<value>4000</value>
</property>
<property>
<name>ipc.server.max.response.size</name>
<value>1048576</value>
</property>
<property>
<name>dfs.client.file-block-storage-locations.timeout.millis</name>
<value>600000</value>
</property>
<property>
<name>dfs.client.failover.connection.retries.on.timeouts</name>
<value>0</value>
</property>
<property>
<name>backup.split.size</name>
<value>1073741824</value>
</property>
<property>
<name>backup.meta.zk.quorum</name>
<value>160-157-0-10:24002,160-157-0-9:24002,167-52-0-13:24002</value>
</property>
<property>
<name>dfs.client.close.ack-timeout</name>
<value>900000</value>
</property>
<property>
<name>dfs.namenode.replication.work.multiplier.per.iteration</name>
<value>2</value>
</property>
<property>
<name>dfs.namenode.delegation.token.renew-interval</name>
<value>86400000</value>
</property>
<property>
<name>dfs.namenode.retrycache.heap.percent</name>
<value>0.03f</value>
</property>
<property>
<name>dfs.image.loader.thread</name>
<value>0</value>
</property>
<property>
<name>dfs.web.authentication.kerberos.principal</name>
<value>HTTP/_HOST@HDFS.COM</value>
</property>
<property>
<name>ipc.client.connect.timeout</name>
<value>10000</value>
</property>
<property>
<name>hadoop.http.authentication.cookie.domain</name>
<value></value>
</property>
<property>
<name>dfs.image.compression.codec</name>
<value>org.apache.hadoop.io.compress.DefaultCodec</value>
</property>
<property>
<name>dfs.client.socket-timeout</name>
<value>600000</value>
</property>
<property>
<name>dfs.namenode.kerberos.internal.spnego.principal</name>
<value>HTTP/_HOST@HDFS.COM</value>
</property>
<property>
<name>dfs.namenode.replication.max-streams-hard-limit</name>
<value>4</value>
</property>
<property>
<name>dfs.image.compress</name>
<value>false</value>
</property>
<property>
<name>dfs.namenode.accesstime.precision</name>
<value>3600000</value>
</property>
<property>
<name>dfs.image.transfer.timeout</name>
<value>600000</value>
</property>
<property>
<name>dfs.client.failover.connection.retries</name>
<value>0</value>
</property>
<property>
<name>dfs.namenode.plugins</name>
<value></value>
</property>
<property>
<name>dfs.namenode.checkpoint.check.period</name>
<value>60</value>
</property>
<property>
<name>dfs.federation.datanode</name>
<value>160-157-0-10:25008,160-157-0-9:25008,167-52-0-13:25008</value>
</property>
<property>
<name>dfs.namenode.retrycache.expirytime.millis</name>
<value>600000</value>
</property>
<property>
<name>dfs.stream-buffer-size</name>
<value>4096</value>
</property>
<property>
<name>dfs.ha.fencing.ssh.connect-timeout</name>
<value>10000</value>
</property>
<property>
<name>dfs.namenode.invalidate.work.pct.per.iteration</name>
<value>0.32</value>
</property>
<property>
<name>dfs.namenode.fs-limits.max-component-length</name>
<value>7999</value>
</property>
<property>
<name>dfs.namenode.service.handler.count</name>
<value>10</value>
</property>
<property>
<name>dfs.ha.fencing.methods</name>
<value>shell(/bin/true)</value>
</property>
<property>
<name>dfs.namenode.enable.retrycache</name>
<value>true</value>
</property>
<property>
<name>dfs.namenode.max.objects</name>
<value>0</value>
</property>
<property>
<name>dfs.bytes-per-checksum</name>
<value>512</value>
</property>
<property>
<name>dfs.cluster.administrators</name>
<value>hdfs supergroup,System_administrator</value>
</property>
<property>
<name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
<value>DEFAULT</value>
</property>
<property>
<name>dfs.encrypt.data.transfer.algorithm</name>
<value>3des</value>
</property>
<property>
<name>dfs.client.read.shortcircuit</name>
<value>true</value>
</property>
<property>
<name>dfs.ha.tail-edits.period</name>
<value>60</value>
</property>
<property>
<name>dfs.ha.namenodes.hacluster</name>
<value>16,17</value>
</property>
<property>
<name>dfs.namenode.https.port</name>
<value>25003</value>
</property>
<property>
<name>dfs.client.failover.sleep.base.millis</name>
<value>500</value>
</property>
<property>
<name>dfs.permissions.superusergroup</name>
<value>supergroup</value>
</property>
<property>
<name>dfs.namenode.fs-limits.max-directory-items</name>
<value>1048576</value>
</property>
<property>
<name>dfs.client.socketcache.capacity</name>
<value>0</value>
</property>
<property>
<name>dfs.namenode.shared.edits.dir</name>
<value>qjournal://${dfs.namenode.journalnode}/hacluster</value>
</property>
<property>
<name>dfs.namenode.edits.dir</name>
<value>/srv/BigData/namenode</value>
</property>
<property>
<name>dfs.ha.log-roll.period</name>
<value>120</value>
</property>
<property>
<name>dfs.image.loader.inode.partition</name>
<value>1048576</value>
</property>
<property>
<name>dfs.ha.namenode.id</name>
<value>16</value>
</property>
<property>
<name>oi.dfs.colocation.file.pattern</name>
<value></value>
</property>
<property>
<name>dfs.namenode.rpc.port</name>
<value>25000</value>
</property>
<property>
<name>dfs.namenode.heartbeat.recheck-interval</name>
<value>300000</value>
</property>
<property>
<name>dfs.namenode.safemode.extension</name>
<value>15000</value>
</property>
<property>
<name>dfs.namenode.name.dir.restore</name>
<value>false</value>
</property>
<property>
<name>dfs.client.failover.sleep.max.millis</name>
<value>15000</value>
</property>
<property>
<name>dfs.support.append</name>
<value>true</value>
</property>
<property>
<name>fs.permissions.umask-mode</name>
<value>022</value>
</property>
<property>
<name>dfs.datanode.socket.reuse.keepalive</name>
<value>-1</value>
</property>
<property>
<name>dfs.namenode.checkpoint.period</name>
<value>1800</value>
</property>
<property>
<name>dfs.ha.automatic-failover.enabled</name>
<value>true</value>
</property>
</configuration>
