<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<configuration>
<property>
<name>dfs.datanode.socket.reuse.keepalive</name>
<value>-1</value>
</property>
<property>
<name>dfs.datanode.kerberos.principal</name>
<value>hdfs/hadoop.hadoop.com@HADOOP.COM</value>
</property>
<property>
<name>dfs.client.socketcache.capacity</name>
<value>0</value>
</property>
<property>
<name>dfs.web.authentication.kerberos.principal</name>
<value>HTTP/_HOST@</value>
</property>
<property>
<name>dfs.namenode.rpc-address.hacluster.19</name>
<value>160-133-0-31:25000</value>
</property>
<property>
<name>dfs.namenode.rpc-address.hacluster.18</name>
<value>160-133-0-22:25000</value>
</property>
<property>
<name>dfs.client.block.write.replace-datanode-on-failure.replication</name>
<value>2</value>
</property>
<property>
<name>dfs.client.socketcache.expiryMsec</name>
<value>900</value>
</property>
<property>
<name>dfs.ha.namenode.id</name>
<value>18</value>
</property>
<property>
<name>dfs.client.close.ack-timeout</name>
<value>120000</value>
</property>
<property>
<name>dfs.namenode.kerberos.principal</name>
<value>hdfs/hadoop.hadoop.com@HADOOP.COM</value>
</property>
<property>
<name>dfs.datanode.http.address</name>
<value>160-133-0-22:25011</value>
</property>
<property>
<name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
<value>DEFAULT</value>
</property>
<property>
<name>dfs.namenode.https.port</name>
<value>25003</value>
</property>
<property>
<name>dfs.datanode.address</name>
<value>160-133-0-22:25009</value>
</property>
<property>
<name>dfs.datanode.data.dir.perm</name>
<value>700</value>
</property>
<property>
<name>dfs.client.failover.max.attempts</name>
<value>10</value>
</property>
<property>
<name>dfs.namenode.kerberos.principal.pattern</name>
<value>*</value>
</property>
<property>
<name>ipc.client.connect.max.retries.on.timeouts</name>
<value>15</value>
</property>
<property>
<name>dfs.ha.namenodes.hacluster</name>
<value>18,19</value>
</property>
<property>
<name>dfs.client.file-block-storage-locations.timeout.millis</name>
<value>600000</value>
</property>
<property>
<name>dfs.namenode.kerberos.https.principal</name>
<value>hdfs/hadoop.hadoop.com@HADOOP.COM</value>
</property>
<property>
<name>dfs.nameservices</name>
<value>hacluster</value>
</property>
<property>
<name>dfs.client.failover.proxy.provider.hacluster</name>
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
<property>
<name>dfs.datanode.kerberos.https.principal</name>
<value>hdfs/hadoop.hadoop.com@HADOOP.COM</value>
</property>
<property>
<name>dfs.webhdfs.enabled</name>
<value>true</value>
</property>
<property>
<name>dfs.blocksize</name>
<value>134217728</value>
</property>
<property>
<name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
<value>true</value>
</property>
<property>
<name>dfs.namenode.rpc.port</name>
<value>25000</value>
</property>
<property>
<name>dfs.client.socket-timeout</name>
<value>600000</value>
</property>
<property>
<name>default.fs.name</name>
<value>hacluster</value>
</property>
</configuration>
